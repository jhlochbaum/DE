# Data Engineering Portfolio

A comprehensive data engineering portfolio demonstrating modern data stack technologies, best practices, and real-world patterns for building scalable data platforms.

## ğŸ¯ Overview

This repository showcases end-to-end data engineering capabilities including:

- **Orchestration**: Apache Airflow for workflow management
- **Distributed Processing**: Apache Spark for large-scale data transformations
- **Stream Processing**: Kafka for real-time event streaming
- **Analytics Engineering**: dbt for transformation and testing
- **Infrastructure as Code**: Terraform for AWS and local infrastructure
- **Data Quality**: Testing and validation patterns
- **Documentation**: Comprehensive documentation and lineage

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Sources                              â”‚
â”‚              APIs â€¢ Databases â€¢ Event Streams                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ingestion Layer                               â”‚
â”‚        Kafka Producers â€¢ Airflow DAGs â€¢ Stream Processors        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Storage Layer (Data Lake)                      â”‚
â”‚           S3/Local Storage â€¢ PostgreSQL â€¢ Delta Lake            â”‚
â”‚             Bronze (Raw) â†’ Silver (Processed) â†’ Gold             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Processing Layer                                 â”‚
â”‚         Apache Spark â€¢ dbt â€¢ Data Quality Checks                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Analytics Layer                                  â”‚
â”‚          Data Marts â€¢ Metrics â€¢ Aggregations                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Tech Stack

### Core Technologies
- **Python 3.11+**: Primary programming language
- **Apache Airflow 2.7.3**: Workflow orchestration
- **Apache Spark 3.5.0**: Distributed data processing
- **Apache Kafka 7.5.0**: Event streaming platform
- **PostgreSQL 15**: Relational database and data warehouse
- **dbt 1.7.3**: Analytics engineering and transformations

### Infrastructure
- **Docker & Docker Compose**: Containerization
- **Terraform**: Infrastructure as Code
- **AWS Services**: S3, Glue, Kinesis, Lambda, CloudWatch

### Development
- **Jupyter**: Interactive development
- **Great Expectations**: Data quality validation
- **pytest**: Testing framework

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop installed and running
- Python 3.11+
- Git

### 1. Clone Repository

```bash
git clone https://github.com/yourusername/DE.git
cd DE
```

### 2. Set Up Environment

```bash
# Copy environment template
cp .env.example .env

# Install Python dependencies (optional for local development)
pip install -r requirements.txt
```

### 3. Start Infrastructure

```bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps
```

### 4. Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Airflow Web UI | http://localhost:8080 | admin / admin |
| Spark Master UI | http://localhost:8081 | - |
| Jupyter Notebook | http://localhost:8888 | Check logs for token |
| PostgreSQL | localhost:5432 | airflow / airflow |
| Kafka | localhost:29092 | - |

## ğŸ“‚ Project Structure

```
DE/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                      # Airflow DAG definitions
â”‚   â”‚   â”œâ”€â”€ 01_simple_etl_pipeline.py
â”‚   â”‚   â”œâ”€â”€ 02_spark_data_processing.py
â”‚   â”‚   â”œâ”€â”€ 03_data_quality_pipeline.py
â”‚   â”‚   â””â”€â”€ 04_kafka_stream_orchestration.py
â”‚   â”œâ”€â”€ plugins/                   # Custom Airflow plugins
â”‚   â””â”€â”€ config/                    # Airflow configurations
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/                      # PySpark job scripts
â”‚   â”‚   â”œâ”€â”€ data_aggregation.py
â”‚   â”‚   â””â”€â”€ data_transformation.py
â”‚   â””â”€â”€ data/                      # Sample data files
â”‚
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer/                  # Kafka producers
â”‚   â”‚   â””â”€â”€ event_producer.py
â”‚   â””â”€â”€ consumer/                  # Kafka consumers
â”‚       â””â”€â”€ event_consumer.py
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # Staging models
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_sales_transactions.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_user_events.sql
â”‚   â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”‚   â””â”€â”€ marts/                # Data marts
â”‚   â”‚       â””â”€â”€ core/
â”‚   â”‚           â”œâ”€â”€ fct_daily_sales.sql
â”‚   â”‚           â”œâ”€â”€ dim_user_metrics.sql
â”‚   â”‚           â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ macros/                   # Custom dbt macros
â”‚   â”œâ”€â”€ tests/                    # Data tests
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ aws/                      # AWS infrastructure
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â””â”€â”€ local/                    # Local Docker infrastructure
â”‚       â””â”€â”€ main.tf
â”‚
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â””â”€â”€ init_db.sql
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”œâ”€â”€ data/                         # Data directories
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ output/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ’¡ Key Features & Examples

### 1. Airflow DAGs

#### Simple ETL Pipeline
Demonstrates basic Extract, Transform, Load pattern:
- Extracts data from PostgreSQL
- Applies transformations using pandas
- Loads results to staging tables

```bash
# Trigger DAG manually
docker exec -it de_airflow_scheduler airflow dags trigger simple_etl_pipeline
```

#### Data Quality Pipeline
Shows data quality checks with branching logic:
- Freshness checks
- Completeness validation
- Volume monitoring
- Alert generation

#### Spark Integration
Orchestrates Spark jobs for large-scale processing:
- Submits PySpark jobs to Spark cluster
- Manages job parameters
- Validates output

### 2. PySpark Jobs

#### Data Aggregation
```bash
docker exec -it de_spark_master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-jobs/data_aggregation.py \
  --output /opt/spark-data/processed
```

Features:
- User-level aggregations
- Product-level metrics
- Time-based analysis
- Partitioned output

#### Data Transformation
Complex transformations with:
- Multi-table joins
- Window functions
- User segmentation
- Advanced analytics

### 3. Kafka Streaming

#### Start Producer
```bash
docker exec -it de_kafka python /app/kafka/producer/event_producer.py \
  --mode continuous \
  --duration 300 \
  --rate 5
```

#### Start Consumer
```bash
docker exec -it de_kafka python /app/kafka/consumer/event_consumer.py \
  --mode continuous \
  --duration 300
```

### 4. dbt Models

#### Run dbt Models
```bash
# Inside Airflow container or locally
dbt run --profiles-dir ./dbt --project-dir ./dbt

# Run specific model
dbt run --models fct_daily_sales --profiles-dir ./dbt --project-dir ./dbt

# Run tests
dbt test --profiles-dir ./dbt --project-dir ./dbt

# Generate documentation
dbt docs generate --profiles-dir ./dbt --project-dir ./dbt
dbt docs serve --profiles-dir ./dbt --project-dir ./dbt
```

### 5. Terraform Infrastructure

#### Local Infrastructure
```bash
cd terraform/local
terraform init
terraform plan
terraform apply
```

#### AWS Infrastructure
```bash
cd terraform/aws

# Initialize
terraform init

# Plan (requires AWS credentials)
terraform plan -var="environment=dev" -var="project_name=de-platform"

# Apply (requires AWS credentials)
terraform apply -var="environment=dev" -var="project_name=de-platform"
```

## ğŸ§ª Testing

### Run Python Tests
```bash
pytest tests/
```

### Run dbt Tests
```bash
dbt test --profiles-dir ./dbt --project-dir ./dbt
```

### Data Quality Checks
Great Expectations integration for automated data validation.

## ğŸ“Š Monitoring & Observability

### Airflow Monitoring
- DAG run history
- Task duration metrics
- Failure alerts
- SLA monitoring

### Data Quality Metrics
- Freshness checks
- Completeness scores
- Volume anomalies
- Schema validation

### Infrastructure Monitoring
- Resource utilization
- Container health checks
- Log aggregation

## ğŸ“ Learning Resources

### Concepts Demonstrated

1. **Medallion Architecture** (Bronze/Silver/Gold)
2. **Slowly Changing Dimensions** (SCD Type 2)
3. **Incremental Processing**
4. **Idempotency in Data Pipelines**
5. **Data Quality Frameworks**
6. **Stream vs Batch Processing**
7. **Data Lineage & Documentation**
8. **Infrastructure as Code**

### Best Practices

- âœ… Version control for all code
- âœ… Comprehensive testing (unit, integration, data quality)
- âœ… Clear documentation and lineage
- âœ… Monitoring and alerting
- âœ… Modular and reusable components
- âœ… Error handling and retry logic
- âœ… Cost optimization strategies
- âœ… Security best practices

## ğŸ› ï¸ Troubleshooting

### Services Not Starting

```bash
# Check logs
docker-compose logs -f [service_name]

# Restart services
docker-compose restart

# Rebuild containers
docker-compose up -d --build
```

### Airflow Webserver Issues

```bash
# Reset Airflow database
docker exec -it de_airflow_webserver airflow db reset

# Create admin user
docker exec -it de_airflow_webserver airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

### Database Connection Issues

```bash
# Test PostgreSQL connection
docker exec -it de_postgres psql -U airflow -d airflow

# Check database status
docker exec -it de_postgres pg_isready
```

## ğŸ“ˆ Performance Optimization

### Spark Tuning
- Partition sizing
- Memory configuration
- Shuffle optimization
- Broadcast joins

### Airflow Optimization
- Parallel task execution
- Connection pooling
- XCom size management
- Task concurrency limits

### Database Optimization
- Indexing strategies
- Query optimization
- Vacuum and analyze
- Partitioning

## ğŸ” Security Considerations

- Secrets management (use environment variables or secret managers)
- Network isolation
- Role-based access control (RBAC)
- Data encryption at rest and in transit
- Audit logging

## ğŸš§ Future Enhancements

- [ ] Add Metabase/Superset for visualization
- [ ] Implement Great Expectations for data quality
- [ ] Add Dagster as alternative orchestrator
- [ ] Implement CDC with Debezium
- [ ] Add Delta Lake for ACID transactions
- [ ] Integrate with cloud data warehouses (Snowflake, BigQuery)
- [ ] Add data lineage visualization
- [ ] Implement feature store

## ğŸ“ License

MIT License - feel free to use this for learning and portfolio purposes.

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or collaboration opportunities, please reach out via [your contact method].

---

**Note**: This is a demonstration project for portfolio purposes. For production use, additional security, monitoring, and optimization would be required.
